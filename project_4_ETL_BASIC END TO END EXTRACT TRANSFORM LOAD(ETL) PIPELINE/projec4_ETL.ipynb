{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project4 - Basic End To End EXTRACT TRANSFORM LOAD(ETL) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "                .appName('Stack Overflow Data Wrangling')\n",
    "                .config('spark.jars','/Users/nanayaw/Desktop/blossom/blossomenv/project_4_ETL/jars/postgresql-42.2.8.jar')\n",
    "                .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "questions = spark.read.csv(\"questions.csv\", header=True, inferSchema=True, escape='\"', multiLine=True)\n",
    "answers = spark.read.csv(\"answers.csv\" , header=True, inferSchema=True, escape='\"', multiLine=True)\n",
    "users = spark.read.csv(\"users.csv\", header=True, inferSchema=True, escape='\"', multiLine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Columns of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions.withColumnRenamed('id', 'questions_id')\n",
    "answers = answers.withColumnRenamed('id', 'answers_id')\n",
    "users = users.withColumnRenamed('id', 'users_id')\n",
    "questions = questions.withColumnRenamed('created_at', 'questions_created_at')\n",
    "answers = answers.withColumnRenamed('created_at', 'answers_created_at')\n",
    "users = users.withColumnRenamed('created_at', 'users_created_at')\n",
    "questions = questions.withColumnRenamed( 'body','questions_body')\n",
    "answers = answers.withColumnRenamed( 'body','answers_body')\n",
    "questions = questions.withColumnRenamed('user_id', 'questions_user_id')\n",
    "answers = answers.withColumnRenamed('user_id', 'answers_user_id')\n",
    "answers = answers.withColumnRenamed('question_id', 'answers_question_id')\n",
    "questions = questions.withColumnRenamed( 'score','questions_score')\n",
    "answers = answers.withColumnRenamed( 'score','answers_score')\n",
    "questions = questions.withColumnRenamed( 'comment_count','questions_comment_count')\n",
    "answers = answers.withColumnRenamed( 'comment_count','answers_comment_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select from User table where location is India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.registerTempTable('n_users')\n",
    "n_users = spark.sql(\"select * from n_users where location like '%India'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove middle towns from location columns with regular expression and split city from country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = n_users.withColumn(\"location\", F.regexp_replace('location', r\"[,]\\s*\\w*\\s*[,]\", ','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users_updated = n_users.withColumn('location', F.split(n_users.location, ',')).select('users_id', 'display_name', 'reputation', 'website_url', \n",
    "                                    'location', 'about_me', 'views', 'up_votes', \n",
    "                                    'down_votes', 'image_url', 'users_created_at', \n",
    "                                    'updated_at', F.element_at(F.col('location'),-2).alias('city')\n",
    "        , F.element_at(F.col('location'), -1).alias('country'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|country|           city|\n",
      "+-------+---------------+\n",
      "|  India|      Bangalore|\n",
      "|  India|      New Delhi|\n",
      "|  India|      Gharaunda|\n",
      "|  India|      New Delhi|\n",
      "|  India|      Jalandhar|\n",
      "|  India|          Surat|\n",
      "|  India|    West Bengal|\n",
      "|  India|           Pune|\n",
      "|  India|         Mumbai|\n",
      "|  India|      Bangalore|\n",
      "|  India|         Mumbai|\n",
      "|  India|      Bangalore|\n",
      "|  India|         Mumbai|\n",
      "|  India|      Hyderabad|\n",
      "|  India| Madhya Pradesh|\n",
      "|  India|           null|\n",
      "|  India|      Bangalore|\n",
      "|  India|    Naya Raipur|\n",
      "|  India|      Bangalore|\n",
      "|  India|     Tamil Nadu|\n",
      "|  India|      Bangalore|\n",
      "|  India|           Pune|\n",
      "|  India|           Pune|\n",
      "|  India|          Delhi|\n",
      "|  India|      Bangalore|\n",
      "|  India|         Mumbai|\n",
      "|  India|    West Bengal|\n",
      "|  India|      Bangalore|\n",
      "|  India|     Tamil Nadu|\n",
      "|  India|      Bangalore|\n",
      "|  India|          Surat|\n",
      "|  India|      Hyderabad|\n",
      "|  India|      Hyderabad|\n",
      "|  India|    West Bengal|\n",
      "|  India|  Uttar Pradesh|\n",
      "|  India|     Tamil Nadu|\n",
      "|  India|         Mumbai|\n",
      "|  India|     Tamil Nadu|\n",
      "|  India|      Alappuzha|\n",
      "|  India|           Pune|\n",
      "|  India|        Gurgaon|\n",
      "|  India|     Tamil Nadu|\n",
      "|  India|        Gurgaon|\n",
      "|  India|    Maharashtra|\n",
      "|  India|     Tamil Nadu|\n",
      "|  India|     Tamil Nadu|\n",
      "|  India|      Bengaluru|\n",
      "|  India|      Hyderabad|\n",
      "|  India|         Mumbai|\n",
      "|  India|      New Delhi|\n",
      "|  India| Madhya Pradesh|\n",
      "|  India|           Pune|\n",
      "|  India|      Ahmedabad|\n",
      "|  India|     Tamil Nadu|\n",
      "|  India|           null|\n",
      "|  India|      Bengaluru|\n",
      "|  India|     Tamil Nadu|\n",
      "|  India|           Pune|\n",
      "|  India|  Uttar Pradesh|\n",
      "|  India|      Bangalore|\n",
      "+-------+---------------+\n",
      "only showing top 60 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_users_updated.select('country','city').show(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join users table and append with city and country to questions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users_questions = n_users_updated.join(questions,n_users_updated.users_id == questions.questions_user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter new joint table where view_count column is 20 or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users_questions.registerTempTable('n_users_questions_tmp')\n",
    "n_users_questions_tmp = spark.sql('select * from n_users_questions_tmp where view_count > 20 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = n_users_questions_tmp.join(answers, n_users_questions_tmp.users_id == answers.answers_user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('users_id', 'int'),\n",
       " ('display_name', 'string'),\n",
       " ('reputation', 'int'),\n",
       " ('website_url', 'string'),\n",
       " ('location', 'array<string>'),\n",
       " ('about_me', 'string'),\n",
       " ('views', 'int'),\n",
       " ('up_votes', 'int'),\n",
       " ('down_votes', 'int'),\n",
       " ('image_url', 'string'),\n",
       " ('users_created_at', 'timestamp'),\n",
       " ('updated_at', 'timestamp'),\n",
       " ('city', 'string'),\n",
       " ('country', 'string'),\n",
       " ('questions_id', 'int'),\n",
       " ('questions_user_id', 'int'),\n",
       " ('title', 'string'),\n",
       " ('questions_body', 'string'),\n",
       " ('accepted_answer_id', 'int'),\n",
       " ('questions_score', 'int'),\n",
       " ('view_count', 'int'),\n",
       " ('questions_comment_count', 'int'),\n",
       " ('questions_created_at', 'timestamp'),\n",
       " ('answers_id', 'int'),\n",
       " ('answers_user_id', 'int'),\n",
       " ('answers_question_id', 'int'),\n",
       " ('answers_body', 'string'),\n",
       " ('answers_score', 'int'),\n",
       " ('answers_comment_count', 'int'),\n",
       " ('answers_created_at', 'timestamp')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select minimum time from updated_at column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.registerTempTable('tmp_results')\n",
    "tmp_results = spark.sql('select min(updated_at) from tmp_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|    min(updated_at)|\n",
      "+-------------------+\n",
      "|2019-01-11 05:02:30|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe to results table in Schema stackoverflow_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.write.format(\"jdbc\").options(\n",
    "   url='jdbc:postgresql://localhost:5434/blossom',\n",
    "   driver='org.postgresql.Driver',\n",
    "   user='Admin',\n",
    "   password='admin',\n",
    "   dbtable='stackoverflow_filtered.results'\n",
    ").save(mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIFFERENCES BETWEEN VIEWS AND MATERIALIZED VIEWS:¶\n",
    "- The difference between a View and a Materialized View is that, views are not stored physically on a disk. While, Materialized views are stored on a disc.\n",
    "- View can be defined as a virtual table created as a result of the query expression. Whereby, Materialized view is a physical copy or snapshot of the base table.\n",
    "- A view is updated always as the query creating view executes each time the view is used. Whereby a Materialized View is updated manually or by applying triggers to it.\n",
    "- Materialized View responds faster than View as the Materialized View is precomputed.\n",
    "- Materialized View utilizes the memory space as it is stored on the disk whereby, the View is just a display therefore it does not require memory space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SQL DATA MANIPULATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 142 cities appeared more than twice in my results table\n",
    "2. There are 1,957 unique created_at dates in my results table\n",
    "3. I will give an award to user name 'Bhuvanesh BS' because he has the highest answer_score of 85 from the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
